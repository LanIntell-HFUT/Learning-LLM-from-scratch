{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee52a5e2-8531-4145-8be7-07f92d715b55",
   "metadata": {},
   "source": [
    "# 2 Byte-Pair Encoding(BPE) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc52a6-4f60-4332-85e6-78be7a869d30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Unicode标准"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc876e-7754-47db-97f5-6ba33f58b5f6",
   "metadata": {},
   "source": [
    "在Python中，可以使用ord()方法将一个单独的Unicode字符转换为它的整数表示；可以使用chr()方法将一个整数Unicode码转换为其对应的字符（字符串类型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43570027-3c86-4d87-b0f2-86417e2f381b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20320"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"你\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dea24d7e-9c83-4080-9569-24e37515e77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(20320)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1f1e4a-fddd-4dc4-b802-d268509544e1",
   "metadata": {},
   "source": [
    "### 问题 (unicode1): 理解Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb2f61-71c0-42e4-b3d6-a5cd73cb4eca",
   "metadata": {},
   "source": [
    "#### (a) Python语句chr(0)会返回什么Unicode字符？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b32bc342-dbaf-4d26-b0c7-f4203f121a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01573d36-b5df-4d2c-9b46-b33cfb847777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"\\x00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80860cca-6222-402b-8716-a3032b843616",
   "metadata": {},
   "source": [
    "答：chr(0)返回\"\\x00\"，表示空字符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a45532-21eb-439c-9ec7-865e35853afd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### (b) 这个字符的字符串表示（\\_\\_repr\\_\\_()）与它的打印表示有什么不同？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7c07628-de96-40d0-8cb3-de198083873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符'\\x00'的字符串表示：\n",
      "s.__repr__():'\\x00'\n",
      "repr(s):'\\x00'\n",
      "字符'\\x00'的打印表示：\n",
      "s.__str__():\u0000\n",
      "str(s):\u0000\n",
      "s:\u0000\n"
     ]
    }
   ],
   "source": [
    "s = \"\\x00\"\n",
    "print(f\"字符{repr(s)}的字符串表示：\")\n",
    "print(f\"s.__repr__():{s.__repr__()}\")\n",
    "print(f\"repr(s):{repr(s)}\")\n",
    "print(f\"字符{repr(s)}的打印表示：\")\n",
    "print(f\"s.__str__():{s.__str__()}\")\n",
    "print(f\"str(s):{str(s)}\")\n",
    "print(f\"s:{s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83de19-f177-4f3b-be8e-ae039958ffcc",
   "metadata": {},
   "source": [
    "答：该字符的字符串表示是未经转义的原始的字符，而其打印表示是经转义后的空字符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df69c99-abeb-41c7-a812-d8f28075d7aa",
   "metadata": {},
   "source": [
    "#### (c) 当这个字符出现在文本里会发生什么？在你的Python解释器中运行以下的语句可能会有所帮助，看看是否与你的预期相符\n",
    "* chr(0)\n",
    "* print(chr(0))\n",
    "* \"this is a test\" + chr(0) + \"string\"\n",
    "* print(\"this is a test\" + chr(0) + \"string\")\n",
    "* Deliverable: A one-sentence response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87de3b3d-2221-43d1-b38a-a6fefbbdcff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16fea86f-018b-4d55-8154-ab75904b3e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9cfae4e-a760-441b-85a5-00d617e6f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6df0a11b-da52-40ab-b1b9-f332df25fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33228d8f-29ad-4b34-8844-8a75dc785dba",
   "metadata": {},
   "source": [
    "答：在print函数中，该字符会被转义为空字符，不显示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947765e4-c586-48ff-afcd-12a2cc65a831",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Unicode编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85ebf2-3d97-4eeb-8098-bceaec3610e0",
   "metadata": {},
   "source": [
    "为了将Unicode字符串编码为UTF-8，我们可以使用Python中的encode()方法，为了获取一个Python bytes对象的潜在字节值，我们可以迭代它（例如，调用list()），最后，我们可以使用decode()方法来解码一个UTF-8字节字符串为一个Unicode字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7af79d26-5b50-4aee-9b02-c80a8e7457e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello! こんにちは!'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0af65fb-54f3-4f36-ad0c-e289fd4bbfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "utf8_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d41a17e9-8d76-4bf6-a3fc-2375d9f35f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "print(type(utf8_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cbd7b3-ca48-43d6-aa20-21ad4da4c5cc",
   "metadata": {},
   "source": [
    "得到编码字符串的字节值（从0到255的整数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "764f2298-e393-4a3e-ac72-1d4b231c0d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 33,\n",
       " 32,\n",
       " 227,\n",
       " 129,\n",
       " 147,\n",
       " 227,\n",
       " 130,\n",
       " 147,\n",
       " 227,\n",
       " 129,\n",
       " 171,\n",
       " 227,\n",
       " 129,\n",
       " 161,\n",
       " 227,\n",
       " 129,\n",
       " 175,\n",
       " 33]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(utf8_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f55fc-5a44-4975-8976-403f833e7885",
   "metadata": {},
   "source": [
    "很显然，一个字节值不一定对应一个Unicode字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4dbf1815-86b7-493c-b527-e38d05713e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b207856-0390-4f75-8e61-41a88b02b9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utf8_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e3063-c6f7-418f-9123-067aa3f6ccd4",
   "metadata": {},
   "source": [
    "直接映射可能不会得到正确的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c18fd16f-7db7-430d-ba5d-5d9f35e1262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104:h\n",
      "101:e\n",
      "108:l\n",
      "108:l\n",
      "111:o\n",
      "33:!\n",
      "32: \n",
      "227:ã\n",
      "129:\n",
      "147:\n",
      "227:ã\n",
      "130:\n",
      "147:\n",
      "227:ã\n",
      "129:\n",
      "171:«\n",
      "227:ã\n",
      "129:\n",
      "161:¡\n",
      "227:ã\n",
      "129:\n",
      "175:¯\n",
      "33:!\n"
     ]
    }
   ],
   "source": [
    "for i in list(utf8_encoded):\n",
    "    print(f\"{i}:{chr(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62d3af76-a2ff-4f6a-a5e2-74d4fbe8eeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12371"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"こ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a653c45e-643a-45de-9286-760de78b8d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello! こんにちは!'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_encoded.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ecbbee-02f4-4ff7-882e-3717a2df6201",
   "metadata": {},
   "source": [
    "把Unicode字符串转换为一个UTF-8字节序列，本质上就是把一个Unicode整数序列（范围是0\\~154997）转换为另一个UTF-8整数序列（范围是0到255）。这说明任何输入文本（因为Unicode本质上已经算涵盖了世界上所有的符号）都可以被表示成由整数0\\~255组成的序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbff8e7-bb2e-4576-9e18-4faa24f966e0",
   "metadata": {},
   "source": [
    "### 问题（unicode2）：Unicode编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a61fcc-e840-4b84-9bc7-ae5f5f7d5d54",
   "metadata": {},
   "source": [
    "#### (a)为什么在训练我们的分词器时，更倾向于使用UTF-8编码的字节，而不是UTF-16或UTF-32？比较这些编码对不同输入字符串的输出可能会有所帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82e8a408-32d0-4a15-a4bb-68718b9afc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf! \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\\xef\\xbc\\x81'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは! 你好！\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "utf8_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ae3f6d7-9028-4fd1-b19f-20732584e8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf0ebc35-efb1-4ec9-b446-aad32f7eb36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00 \\x00`O}Y\\x01\\xff'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "utf16_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb635c68-eff6-4ac2-a266-ad31b8e6d956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utf16_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "036e7755-3fea-470e-84d7-4fd81e950b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00`O\\x00\\x00}Y\\x00\\x00\\x01\\xff\\x00\\x00'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf32_encoded = test_string.encode(\"utf-32\")\n",
    "utf32_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20174a2f-5dce-4ebb-a7b1-5255af3281e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utf32_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30475e8f-b986-4c4b-8a33-ca41e968682d",
   "metadata": {},
   "source": [
    "答：相较于UTF-16和UTF-32，UTF-8能将字符串编码为更短的字节序列，节省空间，同时有能力编码世界上绝大部分的文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb8417-6421-4f4b-8d99-ec8fb81746d4",
   "metadata": {},
   "source": [
    "#### (b)考虑以下（不正确的）函数，该函数旨在将UTF-8字节字符串解码为Unicode字符串。为什么这个函数不正确？请提供一个输入字节字符串的示例，该字符串会产生不正确的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85ed35bf-a4fd-417a-ae90-9eade1b10652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b744df06-d937-4ca5-b97c-6b087d5f7e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sample_encoded = \u001b[33m\"\u001b[39m\u001b[33mこんにちは\u001b[39m\u001b[33m\"\u001b[39m.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_encoded\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "sample_encoded = \"こんにちは\".encode(\"utf-8\")\n",
    "decode_utf8_bytes_to_str_wrong(sample_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0acd57d-fe10-4082-872b-22eff4f5b658",
   "metadata": {},
   "source": [
    "答：因为从Unicode字符映射到UTF-8字节并不一定是1对1的关系，函数只考虑一个字节解码为1个字符的情况了（当然，这对于常见的英文或者数字等可能是正确的）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81c391-d958-4671-b540-1c64b35e1ef1",
   "metadata": {},
   "source": [
    "#### 给出一个无法解码为任何Unicode字符的两字节序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "709e5be4-bee4-4d1f-b787-54c1ea05956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_seq = bytes([0xe3,0xe3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e42d0c9f-dedf-4aef-bb5f-32b3c0be03b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbyte_seq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "byte_seq.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9190905-ad08-4fba-9a28-ced78964ad43",
   "metadata": {},
   "source": [
    "答：[0xe3,0xe3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c4f51-281a-406f-8d62-89f2aa27dfdb",
   "metadata": {},
   "source": [
    "## 2.3 子词编解码(子词token化)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa097f9-a33e-4629-acda-c90504b7dad9",
   "metadata": {},
   "source": [
    "尽管字节级编解码解决了单词级编解码的out-of-vocabulary问题，但字节集编解码将文本转换成字节串会产生一个极长的输入序列，这会导致训练缓慢，长序列也会产生数据的长期依赖。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc98bc1-c8df-46fc-ba4b-5cde7320d8ee",
   "metadata": {},
   "source": [
    "子词编解码(Subword Tokenization)是字节编解码和单词编解码的一种折中。例如，一个字节序列b'the'经常在文本数据中出现，则给它在词典中安排一个1字节的序列而不是3字节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa25767b-a691-4d68-a36d-0e45abf43b47",
   "metadata": {},
   "source": [
    "这种思想产生了BPE Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf16b3-0cd2-42af-a251-361a89c6c2a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.4 BPE编码训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbc833-e560-44df-9b90-248ae1b649fd",
   "metadata": {},
   "source": [
    "BPE编解码器训练分为3个步骤：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da9940-ae7b-4e15-8fab-34d8a748104a",
   "metadata": {},
   "source": [
    "1. 词典初始化：Tokenizer 词表是一个从字节字符串token到整数ID的一对一映射。由于我们要训练一个字节级的BPE tokenizer，我们初始的词典是一个所有字节的集合。由于有256种可能的字节值，所以初始词典大小是256."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071b546-885c-4719-8b53-af122998a9c0",
   "metadata": {},
   "source": [
    "2. 预编解码：为了避免数词频时的一些麻烦，我们需要预编解码语料库。你可以把这看作是语料库上的粗粒度标记化，它可以帮助我们计算成对字符出现的频率。例如，单词\"text\"出现了10次，当我们想统计t和e相邻的次数是多少时，由于我们知道text有t和e相邻，故可以直接把t和e相邻的次数加10。由于我们训练的是字节级BPE，每个token都被表示为一个UTF-8字节序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8afce9-59a6-4314-a4e7-8a034a0bc1e9",
   "metadata": {},
   "source": [
    "原始的BPE实现采用的预编解码手段是简单的空格分割（即s.split('')）。相反，这里我们将采用GPT-2使用的一种基于正则的预编解码器。from github.com/openai/tiktoken/pull/234/files :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06eee1-dced-497a-a2d7-37cfeaa948df",
   "metadata": {},
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac144b0-6340-4cc9-acdd-80d2b2424e9a",
   "metadata": {},
   "source": [
    "使用此预标记器交互式分割一些文本以更好地了解其行为可能很有用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4052ada5-4334-4b11-b6c2-5f497462a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /home/share/anaconda/envs/zyx/lib/python3.12/site-packages (2025.7.34)\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4eaf0df6-3675-4260-9af3-03489c6b1e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0bb61-307a-468b-8317-7f78494745c3",
   "metadata": {},
   "source": [
    "当实际使用时，当构建从预token到它们计数的映射时，应当使用re.finditer以避免保存预编解码的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e9da71eb-e411-420b-8313-d31148235e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(0, 4), match='some'>,\n",
       " <regex.Match object; span=(4, 9), match=' text'>,\n",
       " <regex.Match object; span=(9, 14), match=' that'>,\n",
       " <regex.Match object; span=(14, 16), match=' i'>,\n",
       " <regex.Match object; span=(16, 19), match=\"'ll\">,\n",
       " <regex.Match object; span=(19, 23), match=' pre'>,\n",
       " <regex.Match object; span=(23, 24), match='-'>,\n",
       " <regex.Match object; span=(24, 32), match='tokenize'>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = re.finditer(PAT, \"some text that i'll pre-tokenize\")\n",
    "list(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c8a89-8e58-48ac-a266-5ee2c19c05af",
   "metadata": {},
   "source": [
    "3. 计算BPE合并：在高层次，BPE算法迭代地计数每一对字节，并识别频率最高的字节对（如\"A\", \"B\"）,每一次这一对词频最高的字节对出现，则合并它们（即用一个新的token\"AB\"替换它们），这一个新的token加入到词典中（例如，本来的词典是0到255，这个新的token可以是256）。最终，经过BPE训练后的词典大小为“**初始词典大小（256）+ BPE合并操作的执行次数 + 特殊tokens的数量**” "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e2378-9496-4f40-a604-84d9c86358d9",
   "metadata": {},
   "source": [
    "为了提高BPE训练的效率，我们不考虑跨越预令牌边界的配对。当计算合并时，通过优先选择字典上较大的配对来确定配对频率中的联系。例如:如果字节对 (“A”, “B”), (“A”, “C”), (“B”, “ZZ”), 和 (“BA”, “A”) 都有最高的频率, 我们将合并 (“BA”, “A”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "07067c3c-49c2-4137-9e77-69748c7e278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89addb86-cdaf-48d2-8d7e-e98701f5c103",
   "metadata": {},
   "source": [
    "特殊tokens：有时会加入一些特殊tokens来编码元数据（例如<|endoftext|>），这些字符不会被分解为多个tokens，并且要被加入到词典中，有固定的tokenID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc8f829-1524-4bdc-9a9a-a46a25b2628f",
   "metadata": {},
   "source": [
    "### 示例（bpe_example）:BPE训练示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5678f-a7ff-423c-9d73-ee8bc78d924c",
   "metadata": {},
   "source": [
    "以下是一个原始论文中的BPE训练示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5883071-c5e6-4293-a551-694cfa3aa6d7",
   "metadata": {},
   "source": [
    "假设语料库中有如下文本：\n",
    "low low low low low  \n",
    "lower lower widest widest widest  \n",
    "newest newest newest newest newest newest  \n",
    "词典有特殊token:<|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64ed69-96f3-4e21-a165-82a56b70b24e",
   "metadata": {},
   "source": [
    "词典：使用特殊token<|endoftext|>和256个字节值初始化词典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238793c8-9cfc-47c7-8cad-794193ac3394",
   "metadata": {},
   "source": [
    "预编解码：为了简化，假设使用根据空格分割的预编解码。  \n",
    "预编解码和计数后，得到频率表：  \n",
    "{low: 5, lower: 2, widest: 3, newest: 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56785cfd-4875-45ae-910d-84375e4639e1",
   "metadata": {},
   "source": [
    "很容易把这个表表示为一个字典形式：dict[tuple[bytes], int],例如：{(l,o,w):5, {(l,o,w,e,r):2},...}。  \n",
    "注意：在Python中，即使是单个字节也是bytes类型，在Python中没有byte类型来表示单个字节，正如没有char类型来表示单个字符。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6670fcb-3c45-4333-9c37-f0412fcdc33b",
   "metadata": {},
   "source": [
    "合并：  \n",
    "我们首先看每一个连续的字节对，并对它们出现的次数求和： {lo: 7, ow: 7, we: 8, er: 2, wi: 3, id: 3, de: 3, es: 9, st: 9, ne: 6, ew: 6}，字节对“es”和“st”都是最多的，并且平手了。于是我们选择字典序更大的“st”。合并预tokens中的st，得到： {(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd797084-2e96-4740-84bd-1a70ed252823",
   "metadata": {},
   "source": [
    "在第二轮，同理得到（e,st）是词频最高的字节对（出现9次），把他们合并得到： {(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,est): 3, (n,e,w,est): 6}  \n",
    "继续这个过程，最终我们会得到的合并序列是：['s t', 'e st', 'o w', 'l ow', 'w est', 'n e','ne west', 'w i', 'wi d', 'wid est', 'low e', 'lowe r']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9af0c-4cf7-4459-a06f-4086c9e47987",
   "metadata": {},
   "source": [
    "如果我们采用6个合并，我们有 ['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']   \n",
    "我们的词典最终会变成： [<|endoftext|>, [...256 BYTE CHARS], st, est, ow, low, west, ne]  \n",
    "有了这个词典和合并集合，单词“newest”会被编码成[ne,west]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42209504-ecd7-448f-a15e-2b2b1ee78142",
   "metadata": {},
   "source": [
    "## 2.5 BPE tokenizer训练实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86df20-eb11-44c6-8889-5ce16b1f6957",
   "metadata": {},
   "source": [
    "在TinyStories数据集上训练一个字节级BPE tokenizer。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c11678-33f5-4155-89d6-675c07701ddb",
   "metadata": {},
   "source": [
    "### 小提示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b078e42-fdab-47a0-aa86-a413c784f1e7",
   "metadata": {},
   "source": [
    "预分词并行化 你会发现，预分词步骤是一个主要瓶颈。你可以通过使用内置库multiprocessing对代码进行并行化来加速预分词。具体来说，我们建议在预分词的并行实现中，对语料库进行分块，同时确保分块边界出现在特殊标记的开头。你可以直接使用以下链接中的示例代码来获取分块边界，然后利用这些边界将工作分配到各个进程中：  \n",
    "https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py  \n",
    "这种分块将始终有效，因为我们永远不想跨文档边界合并。为了作业的目的，你总是可以这样拆分。不要担心接收不包含<|endoftext|>的非常大的语料库的边缘情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c04cf-23eb-467f-a8a5-579d9acf1606",
   "metadata": {},
   "source": [
    "删除所有特殊标记 在使用正则表达式模式（使用re.finditer）运行预标记化之前，您应该从语料库（或块，如果使用并行实现）中删除所有特殊标记。确保在特殊标记上进行拆分，这样它们所分隔的文本之间就不会发生合并。例如，如果你有一个像[Doc 1]<|endoftext|>[Doc 2]这样的语料库（或块），你应该在特殊标记<|endoptext|>上拆分，并分别对[Doc 1]和[Doc 2]进行预标记，这样就不会在文档边界上发生合并。这可以通过使用re.split，并使用\"|\".join（special_tokens）作为分隔符来完成（请谨慎使用re.escape，因为|可能出现在特殊标记中）。test_train_bpe_special_token将对此进行测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d5742-eb91-4dc7-8d34-aaac03279494",
   "metadata": {},
   "source": [
    "优化合并步骤  上述程式化示例中BPE训练的天真实现很慢，因为对于每次合并，它都会迭代所有字节对以识别最频繁的对。但是，每次合并后唯一发生变化的对计数是与合并对重叠的计数。因此，通过索引所有对的计数并逐步更新这些计数，而不是显式迭代每对字节来计算对频率，可以提高BPE训练速度。使用此缓存过程可以显著提高速度，尽管我们注意到BPE训练的合并部分在Python中是不可并行的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02621163-da22-4fd2-a781-c8859147df9d",
   "metadata": {},
   "source": [
    "### 问题（train_bpe）: BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c204c15-9f10-467f-9529-2434997fe689",
   "metadata": {},
   "source": [
    "写一个函数，给定一个输入文本文件的地址，训练一个字节级BPE tokenizer。该BPE训练函数至少应包含以下输入参数：  \n",
    "* input_path : str 指向一个有BPE tokenizer训练数据文件的路径\n",
    "* vocab_size : int 一个正整数，定义最终词典的最大尺寸（包括初始字节词典，由合并过程中产生的词典项和任何特殊tokens）\n",
    "* special_tokens : list[str] 加入到词典中的一个字符串列表。这些特殊tokens不影响BPE训练  \n",
    "你的BPE训练函数应当返回结果词典和合并集：\n",
    "* vocab : dict[int, bytes] 编解码器词典，一个从int（词典中的token ID）到bytes（token bytes）的映射\n",
    "* merges: list[tuple[bytes, bytes]] 一个在BPE训练中产生的合并的列表。每个列表项都是一个bytes的元组（\\<token1\\>,\\<token2\\>），表示\\<token1\\>被合并到\\<token2\\>中。合并应当按照创建的顺序保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f6992-d2c8-42e7-abd2-cc92304d3689",
   "metadata": {},
   "source": [
    "答：见bpe_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e5883-736c-4f58-90ef-912208aaa141",
   "metadata": {},
   "source": [
    "### 问题（train_bpe_tinystories）: 在TinyStories数据集上训练BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d915c0b-6c94-4101-bf25-88efc4739181",
   "metadata": {},
   "source": [
    "#### (a)在TinStock数据集上训练字节级BPE标记器，最大词汇量为10000。  \n",
    "请确保将TinyStories的特殊标记添加到词汇表中。序列化生成的词汇表并合并到磁盘以供进一步检查。\n",
    "训练需要多少小时和内存？词汇表中最长的标记是什么？这有道理吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8ee15-9b8d-4c5e-8bda-4ff915006c50",
   "metadata": {},
   "source": [
    "资源限制： ≤ 30 minutes (no GPUs), ≤ 30GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f588ff-3e30-4606-a6ea-8f9b67e9452f",
   "metadata": {},
   "source": [
    "提示：应当能够在预编解码的过程中使用multiprocessing进行BPE训练的情况下，得到两分钟以下的时间。并得到如下事实：  \n",
    "* <|endoftext|>标记用于分隔数据文件中的文档\n",
    "* 在应用BPE合并之前，将<|endoftext|>标记作为特例处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0746d-ec5c-498c-8faa-55dcb035b280",
   "metadata": {},
   "source": [
    "答：代码和执行见BPETrain.ipynb，执行时间为993.8秒"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71bd4f-b63e-4b89-bc7a-d9a4d9c00fae",
   "metadata": {},
   "source": [
    "#### (b) 分析你的代码。tokenizer训练过程的哪个部分花费的时间最多？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3911ed-cc8a-4bfa-9a0e-7389498583b5",
   "metadata": {},
   "source": [
    "### 问题（train_bpe_expts_owt）: 在OpenWebText数据集上训练BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df91e9-76c9-42bb-b053-861a901383b8",
   "metadata": {},
   "source": [
    "#### (a) 在OpenWebText数据集上训练字节级BPE标记器，最大词汇量为32000。\n",
    "序列化生成的词汇表并合并到磁盘以供进一步检查。词汇表中最长的标记是什么？这有道理吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20960c8-aa9b-4958-8dc4-0a2a2be89c5a",
   "metadata": {},
   "source": [
    "资源限制: ≤ 12 hours (no GPUs), ≤ 100GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec792fb-3ff4-4501-b581-1cbc1cbdf059",
   "metadata": {},
   "source": [
    "#### (b) 比较和对比您在TinyStories 和OpenWebText上训练得到的Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2862479d-524d-48c2-b7a6-f8a5f5ff053a",
   "metadata": {},
   "source": [
    "## 2.6 BPE Tokenizer:编码和解码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c25df7-ba00-4c4a-9865-2e0551e52d7f",
   "metadata": {},
   "source": [
    "我们之前实现了一个BPE tokenizer，可以取得tokenizer词汇表和一个merges列表，接下来我们将实现一个可以加载给定词汇表和一列merges的BPE tokenizer，并使用它们来编码（从文本到Token IDs）和解码（从Token Ids到文本）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7e525-7e8c-4f50-843d-1c4aac0d247b",
   "metadata": {},
   "source": [
    "### 2.6.1 编码文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19ed8c-e156-4018-84fb-a02072b0ec62",
   "metadata": {},
   "source": [
    "#### Step 1：预编码  \n",
    "就像在训练BPE时做的那样预编码。然后，我们将在每个预token内部将这些字节合并为词汇表中的元素，独立处理每个预token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b7056-ab41-43d9-95f5-106a337fe6a9",
   "metadata": {},
   "source": [
    "#### Step 2：应用merges  \n",
    "采用由BPE训练期间产生的merges，把它应用到我们的预token上，以和训练时创造的merges相同的顺序进行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ac050-c02e-48b5-b8cf-0afbdea9e20e",
   "metadata": {},
   "source": [
    "#### 示例：(bpe_encoding): BPE Encoding Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61accbf4-06bb-418c-87e1-6cf1497c422e",
   "metadata": {},
   "source": [
    "例如，假设我们的输入字符串是'the cat ate'，我们的词汇表是：  \n",
    "                                                {  \n",
    "                                                0: b' ',   \n",
    "                                                1: b'a',   \n",
    "                                                2: b'c',   \n",
    "                                                3: b'e',   \n",
    "                                                4: b'h',   \n",
    "                                                5: b't',   \n",
    "                                                6: b'th',   \n",
    "                                                7: b' c',   \n",
    "                                                8: b' a',    \n",
    "                                                9: b'the',   \n",
    "                                                10: b'at'  \n",
    "                                                }  \n",
    "我们学到的merges是：  \n",
    "[  \n",
    "(b't', b'h'),   \n",
    "(b' ', b'c'),   \n",
    "(b' ', 'a'),   \n",
    "(b'th', b'e'),  \n",
    "(b' a', b't')  \n",
    "]  \n",
    "首先，我们的预编码器会将字符串拆分为['the', ' cat', ' ate']。  \n",
    "然后，我们会逐一看每一个预token，并应用BPE merges  \n",
    "  \n",
    "第一个预token\"the\" 被首先表示为[b't',b'h',b'e']。查看我们的merges列表，我们发现第一个可应用的merge是(b't', b'h'), 所以应用它来转换我们的预token为[b'th',b'e']。  \n",
    "然后，继续回到merges列表查找下一个可应用的merge是(b'th', b'e')，将预token转换为[b'the']。  \n",
    "最后，我们发现merges列表中已经没有能应用于字符串的了(因为整个预token已经被合并成为单独的token)，至此我们已经应用了BPE merging，对应的整数序列是9\n",
    "\n",
    "对剩下的预tokens继续应用BPE merging，我们看到' cat'被表示成 [b' c', b'a', b't'] 成为整数序列[7,1,5]。最后一个预token' ate'是[b' at', b'e']，整数序列[10,3]。  \n",
    "\n",
    "因此，我们最终的编码结果是 [9, 7, 1, 5, 10, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4c366-bfd9-433d-846d-6139c5de46ff",
   "metadata": {},
   "source": [
    "#### 特殊tokens  \n",
    "你实现的tokenizer应当能够正确处理用户定义的特殊tokens（当构造tokenizer时提供）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62dd2db-2d98-488d-8704-2920ec89ad13",
   "metadata": {},
   "source": [
    "#### 存储考虑  \n",
    "假设我们想要对一个无法完全装入内存的大型文本文件进行分词。为了高效地处理这个大文件（或任何其他数据流），我们需要将其分解为可管理的块，并依次处理每个块，这样内存复杂度就是常量，而不是随文本大小线性增长。在这样做时，我们需要确保词元不会跨越块的边界，否则我们会得到与在内存中对整个序列进行分词的简单方法不同的分词结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73946ed-a358-46fc-b3cb-aeb51342650a",
   "metadata": {},
   "source": [
    "### 2.6.2 解码文本  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32b272-31b4-41e4-af28-3e18f46ead4d",
   "metadata": {},
   "source": [
    "为了把一串整数token IDs序列解码为原文本，我们可以简单查找每一个ID对应的条目（一个字节序列bytes），把它们拼接到一起，然后把bytes解码为Unicode字符串。  \n",
    "注意，有的输入IDs不能保证映射为一个有效的Unicode字符串（由于用户可以输入任意的整数序列）。在这种情况下输入token IDs不能产生一个Unicode字符串，应当替换为官方的Unicode替换字符U+FFFD来替换这些格式错误的字节。bytes.decode方法中的error参数控制Unicode解码错误如何处理。使用errors='replace'将会自动替换为替换标识。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ceeb5-2a27-4456-9c2e-7100592a1da8",
   "metadata": {},
   "source": [
    "### 问题(tokenizer) :实现tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b43dd-bb4e-406f-acfa-d6db67fd7c25",
   "metadata": {},
   "source": [
    "实现一个Tokenizer类，该类在给定词汇表和合并列表的情况下，将文本编码为整数ID，并将整数ID解码为文本。你的分词器还应支持用户提供的特殊词元（如果它们尚未存在于词汇表中，则将其追加到词汇表中）。我们建议使用以下接口："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff3e24-ef6d-4944-9818-ed6c89f4d978",
   "metadata": {},
   "source": [
    "def \\_\\_init\\_\\_(self, vocab, merges, special_tokens=None): 根据给定的词汇表、merges列表和（可选的）特殊tokens构建一个tokenizer。这个方法应当接收以下参数：\n",
    "* vocab: dict[int, bytes]\n",
    "* merges: list[tuple[bytes, bytes]]\n",
    "* special_tokens: list[str] | None = None\n",
    "  \n",
    "def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None):从一个序列化的词汇表和一个merges列表以及（可选的）特殊tokens构建并返回一个Tokenizer的类方法（和你的BPE训练器代码输出有相同的形式）。这个方法应当接收以下参数：  \n",
    "* vocab_filepath: str\n",
    "* merges_filepath: str\n",
    "* special_tokens: list[str] | None = None\n",
    "\n",
    "def encode(self, text: str) -> list[int]: 将一个输入文本编码为token IDs的序列。  \n",
    "\n",
    "def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]: 给定一个字符串的可迭代对象（例如一个Python的文件handle），返回一个惰性生成token ID 的生成器。这对于我们无法直接加载到内存中的大文件进行内存高效分词是必需的。  \n",
    "\n",
    "def decode(self, ids: list[int]) -> str: 将一个token IDs序列解码为文本  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4684f-df92-4d28-9360-b89d0c66e8c2",
   "metadata": {},
   "source": [
    "答：见tokenizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d241d-fae7-40d7-94a4-3c6961a26d5b",
   "metadata": {},
   "source": [
    "## 2.7 实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb3385-c5d5-436f-999f-66894ed25cf7",
   "metadata": {},
   "source": [
    "### 问题（tokenizer_experiments）:tokenizer实验  \n",
    "(a) 从TinyStories和OpenWebText中各采样10个文档。使用你之前训练的TinyStories和OpenWebText分词器（词汇表大小分别为10K和32K），将这些采样文档编码为整数ID。每个分词器的压缩比（字节/词元）是多少？  \n",
    "  \n",
    "(b) 如果你使用TinyStories分词器来分词OpenWebText样本会发生什么？比较压缩比和/或定性地描述会发生什么。  \n",
    "  \n",
    "(c) 估算你的分词器的吞吐量（例如，以字节/秒为单位）。分词Pile数据集（825GB文本）需要多长时间？  \n",
    "  \n",
    "(d) 使用你的TinyStories和OpenWebText分词器，将各自的训练和开发数据集编码为整数词元ID序列。我们稍后将使用这个来训练我们的语言模型。我们建议将词元ID序列化为uint16数据类型的NumPy数组。为什么uint16是一个合适的选择？  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ecaba-1e8b-42a9-8e0f-d594201e9ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010565b4-352d-4e89-a24f-65657fecb5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
