{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39aceed0-430e-42b9-9143-12776382085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import os\n",
    "from typing import BinaryIO\n",
    "\n",
    "# 初始化词表\n",
    "def init_vocabulary(vocab_size=256) -> dict:\n",
    "    init_vocab = {chr(i):i for i in range(vocab_size)} # 初始词典有256种可能的字节值\n",
    "    return init_vocab\n",
    "    \n",
    "# 去除特殊tokens，便于统计词频\n",
    "def remove_special_tokens(raw_text,special_tokens = [\"<|endoftext|>\"]):\n",
    "    for sp_token in special_tokens:\n",
    "        raw_text = raw_text.replace(sp_token, \"\")\n",
    "    return raw_text\n",
    "\n",
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "# 预编解码\n",
    "def pre_tokenization(raw_text : str,\n",
    "                     PAT=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n",
    "                     special_tokens=[\"<|endoftext|>\"]) -> re.Scanner:\n",
    "    removed_text = remove_special_tokens(raw_text,special_tokens)\n",
    "    return re.finditer(PAT,removed_text)\n",
    "\n",
    "\n",
    "# 统计经预编解码后的文本的词频\n",
    "def count_word_freq(it : re.Scanner) -> dict[str : int]:\n",
    "    word_freq = {}\n",
    "    for i in it:\n",
    "        word_freq[i.group()] = word_freq.get(i.group(),0) + 1\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    把文件分割成可以被单独计数的块，如果边界重叠，可能返回比预期更少的块\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "    # print(f\"初步简单规划的区块边界为:{chunk_boundaries}\")\n",
    "    # print(\"第一个区块无需检查\")\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    # 目的是确保区块边界开始于特殊token\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        # print(f\"第{bi+1}/{len(chunk_boundaries) - 1}个区块，初始边界猜测位置为：{initial_position}\")\n",
    "        file.seek(initial_position)  # 在猜测的边界开始循环检测(EOF/特殊token)\n",
    "        \n",
    "        while True: # 每一次循环的操作对象都是一个mini_chunk\n",
    "            mini_chunk = file.read(mini_chunk_size)  # 读取一个 mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                # print(\"抵达文件末尾，结束\")\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # 在这个mini chunk里寻找特殊token，found_at返回的是特殊token在mini chunck中的相对位置\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            \n",
    "            if found_at != -1: # 如果找到了特殊token\n",
    "                # print(f\"在{found_at}处发现特殊token, 将区块开始位置置于该特殊token之后\")\n",
    "                chunk_boundaries[bi] = initial_position + found_at # 把这个边界的实际初始位置后移found_at个单位，这样正好能把边界设在特殊token的后面。\n",
    "                break\n",
    "\n",
    "            # print(\"forward\") # 如果在该mini chunk中未发现特殊token，则继续检查下一个mini chunk\n",
    "            initial_position += mini_chunk_size # 始终保持边界指针位于当前mini chunk的开头，以便于用相对位置来移动指针\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "# 统计经预编解码后的文本的词频(多进程版本)\n",
    "def process_chunk(start_end_pair, file_path):\n",
    "    \"\"\"\n",
    "    处理单个文件块并计算词频。\n",
    "    此函数由每个工作进程独立执行。\n",
    "\n",
    "    Args:\n",
    "        start_end_pair (tuple): 包含块的起始和结束字节位置的元组 (start, end)。\n",
    "        file_path (str): 要读取的文件的路径。\n",
    "\n",
    "    Returns:\n",
    "        Counter: 该块内单词及其频率的 Counter 对象。\n",
    "    \"\"\"\n",
    "    start, end = start_end_pair\n",
    "    # 每个进程必须独立打开文件，因为文件对象不能在进程间共享。\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        # 对你的数据块运行预分词并存储词频\n",
    "        it = pre_tokenization(chunk)\n",
    "        word_count = count_word_freq(it)\n",
    "        return Counter(word_count)\n",
    "\n",
    "def parallel_word_count(file_path, num_processes=4):\n",
    "    \"\"\"\n",
    "    并行计算文件中单词的频率。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 目标文件的路径。\n",
    "        num_processes (int): 要使用的进程数。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含所有单词及其总频率的字典。\n",
    "    \"\"\"\n",
    "    # 1. 在主进程中确定所有数据块的边界\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "    # 创建一个 (start, end) 元组的列表，供工作进程处理\n",
    "    chunks = list(zip(boundaries[:-1], boundaries[1:]))\n",
    "\n",
    "    # 2. 创建一个工作进程池\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        # 使用 functools.partial 创建一个新函数\n",
    "        # 这样可以将 file_path 参数固定，pool.map 调用时只需传递变化的 chunk 参数\n",
    "        worker_func = partial(process_chunk, file_path=file_path)\n",
    "\n",
    "        # 3. 将任务分配给进程池并收集结果\n",
    "        # pool.map 会将 chunks 列表中的每个元素作为参数传递给 worker_func，并并行执行\n",
    "        list_of_counts = pool.map(worker_func, chunks)\n",
    "\n",
    "    # 4. 合并所有进程返回的局部结果\n",
    "    final_word_freq = Counter()\n",
    "    for count_obj in list_of_counts:\n",
    "        final_word_freq.update(count_obj)\n",
    "        \n",
    "    return dict(final_word_freq)\n",
    "    \n",
    "# 将词频字典的键（字符串类型）转换为元组，便于后续统计字节对词频\n",
    "def word_freq_str2tuple(word_freq : dict[str:int]) -> dict[tuple:int]:\n",
    "    tuple_word_freq = {}\n",
    "    for k,v in word_freq.items():\n",
    "        tuple_word_freq[tuple(k)] = v\n",
    "    return tuple_word_freq\n",
    "\n",
    "# 统计字节对词频\n",
    "def count_bytes_pair_freq(tuple_word_freq : dict[tuple:int]) -> dict[tuple[str,str]:int]:\n",
    "    # print(f\"输入:{tuple_word_freq}\")\n",
    "    bytes_pair_freq = {}\n",
    "    for k,v in tuple_word_freq.items():\n",
    "        if len(k)==1: continue # 由于只统计字节对的词频，所以只有一个字节的元组可以不考虑\n",
    "        # 计算字节对词频\n",
    "        for i in range(len(k)-1):\n",
    "            combined_tuple = (k[i],k[i+1])\n",
    "            bytes_pair_freq[combined_tuple] = bytes_pair_freq.get(combined_tuple,0) + v\n",
    "    return bytes_pair_freq\n",
    "\n",
    "# 找出词频最高的字节对并加入词表\n",
    "def find_max_bytes_pair(bytes_pair_freq : dict[tuple[str,str]:int]) -> tuple[str,str]:\n",
    "    max_value = max(bytes_pair_freq.values()) # 最高的词频数\n",
    "    # print(f\"max_value:{max_value}\")\n",
    "    max_items = {key: value for key, value in bytes_pair_freq.items() if value == max_value}\n",
    "    # print(f\"max_items:{max_items}\")\n",
    "    max_items_list = list(max_items.keys())\n",
    "    word_tuple_to_add = max(max_items_list) # 要加入词表的新词\n",
    "    return word_tuple_to_add\n",
    "\n",
    "def add_to_vocab(raw_vocab : dict[str:int], new_key:str) -> dict[str:int]:\n",
    "    new_vocab = raw_vocab.copy()\n",
    "    # print(len(raw_vocab))\n",
    "    # print(new_key)\n",
    "    new_vocab[new_key] = len(raw_vocab)\n",
    "    # print(len(new_vocab))\n",
    "    \n",
    "    return new_vocab\n",
    "\n",
    "# 修改tuple_word_freq，合并新加入的字节对\n",
    "def update_tuple_word_freq(raw_tuple_word_freq : dict[tuple:int], word_tuple_to_add:tuple[str,str],new_key:str) -> dict[tuple:int] :\n",
    "    new_tuple_word_freq = {}\n",
    "    for k,v in raw_tuple_word_freq.items():\n",
    "        if len(k)==1:\n",
    "            continue\n",
    "        # 寻找需要合并的字节对\n",
    "        for i in range(len(k)-1):\n",
    "            if k[i] == word_tuple_to_add[0] and k[i+1] == word_tuple_to_add[1]: # 合并\n",
    "                ls = list(k)\n",
    "                ls[i] = new_key\n",
    "                del ls[i+1]\n",
    "                new_tuple_word_freq[tuple(ls)]=v\n",
    "                break\n",
    "            if i == len(k)-2:\n",
    "                new_tuple_word_freq[k] = v    \n",
    "    return new_tuple_word_freq\n",
    "\n",
    "def bpe_merge(init_vocab,file_path,vocab_size,special_tokens=[\"<|endoftext|>\"]) -> dict[bytes:int]:\n",
    "    # 统计经预分词处理后的词频\n",
    "    # word_freq = count_word_freq(it)\n",
    "    word_freq = parallel_word_count(file_path)\n",
    "    # 将词频字典键的字符串转换为元组\n",
    "    tuple_word_freq = word_freq_str2tuple(word_freq)\n",
    "\n",
    "    new_vocab = init_vocab.copy()\n",
    "    \n",
    "    # 计算要合并的次数(由于最后还要加上特殊token，所以合并次数要减去特殊tokens的数量)\n",
    "    t = vocab_size - len(init_vocab) - len(special_tokens)\n",
    "    # print(f\"将进行{t}次合并\")\n",
    "    merges = [] # 记录每次合并的tuple[bytes:bytes]\n",
    "    # 合并t次\n",
    "    for i in range(t):\n",
    "        # print(f\"第{i+1}次合并开始\")\n",
    "        # 统计字节对词频\n",
    "        bytes_pair_freq = count_bytes_pair_freq(tuple_word_freq)\n",
    "        # 找出词频最高的字节对\n",
    "        word_tuple_to_add = find_max_bytes_pair(bytes_pair_freq)\n",
    "        # print(word_tuple_to_add)\n",
    "        # 转换成字节串，加入到merges中\n",
    "        word_tuple_to_merge = tuple(s.encode('utf-8') for s in word_tuple_to_add)\n",
    "        merges.append(word_tuple_to_merge)\n",
    "        \n",
    "        new_key = word_tuple_to_add[0] + word_tuple_to_add[1]\n",
    "        # print(new_key)\n",
    "        # 加入词表\n",
    "        new_vocab = add_to_vocab(new_vocab, new_key)\n",
    "        # 修改tuple_word_freq，合并新加入的字节对\n",
    "        tuple_word_freq = update_tuple_word_freq(tuple_word_freq,word_tuple_to_add,new_key)\n",
    "        # print(f\"输出:{tuple_word_freq}\")\n",
    "    assert len(new_vocab) == vocab_size - len(special_tokens), f\"length of new_vocab is {len(new_vocab)},while expected is {vocab_size - len(special_tokens)}\"\n",
    "    \n",
    "    # 加上特殊tokens\n",
    "    for sp_tok in special_tokens:\n",
    "        new_vocab[sp_tok] = len(new_vocab)\n",
    "    assert len(new_vocab) == vocab_size\n",
    "\n",
    "    # 把字符串转换为字节串\n",
    "    new_vocab_bytes = {key.encode('utf-8'): value for key, value in new_vocab.items()}\n",
    "    reversed_dict = {v: k for k, v in new_vocab_bytes.items()}\n",
    "    return reversed_dict,merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a90caa74-dc9e-44a2-84e9-425c8fea6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     raw_text = f.read()\n",
    "#     f.close()\n",
    "\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "\n",
    "init_vocab = init_vocabulary()\n",
    "# sp_tokens_removed_text = remove_special_tokens(raw_text, special_tokens)\n",
    "# it = pre_tokenization(sp_tokens_removed_text[:1000])\n",
    "vocab,merges = bpe_merge(init_vocab, file_path, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0dd84c3-c928-4541-9cf7-e2bc75b6f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPE_tokenizer_training(input_path=\"./data/TinyStoriesV2-GPT4-valid.txt\", \n",
    "                           vocab_size=300, special_tokens=[\"<|endoftext|>\"]) :\n",
    "    init_vocab = init_vocabulary()\n",
    "    return bpe_merge(init_vocab, input_path, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bb930b7e-02a4-46c9-8a94-c2b523333c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,merges = BPE_tokenizer_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8c25231-bc15-4906-a795-1a40c4793a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'\\x00': 0,\n",
       " b'\\x01': 1,\n",
       " b'\\x02': 2,\n",
       " b'\\x03': 3,\n",
       " b'\\x04': 4,\n",
       " b'\\x05': 5,\n",
       " b'\\x06': 6,\n",
       " b'\\x07': 7,\n",
       " b'\\x08': 8,\n",
       " b'\\t': 9,\n",
       " b'\\n': 10,\n",
       " b'\\x0b': 11,\n",
       " b'\\x0c': 12,\n",
       " b'\\r': 13,\n",
       " b'\\x0e': 14,\n",
       " b'\\x0f': 15,\n",
       " b'\\x10': 16,\n",
       " b'\\x11': 17,\n",
       " b'\\x12': 18,\n",
       " b'\\x13': 19,\n",
       " b'\\x14': 20,\n",
       " b'\\x15': 21,\n",
       " b'\\x16': 22,\n",
       " b'\\x17': 23,\n",
       " b'\\x18': 24,\n",
       " b'\\x19': 25,\n",
       " b'\\x1a': 26,\n",
       " b'\\x1b': 27,\n",
       " b'\\x1c': 28,\n",
       " b'\\x1d': 29,\n",
       " b'\\x1e': 30,\n",
       " b'\\x1f': 31,\n",
       " b' ': 32,\n",
       " b'!': 33,\n",
       " b'\"': 34,\n",
       " b'#': 35,\n",
       " b'$': 36,\n",
       " b'%': 37,\n",
       " b'&': 38,\n",
       " b\"'\": 39,\n",
       " b'(': 40,\n",
       " b')': 41,\n",
       " b'*': 42,\n",
       " b'+': 43,\n",
       " b',': 44,\n",
       " b'-': 45,\n",
       " b'.': 46,\n",
       " b'/': 47,\n",
       " b'0': 48,\n",
       " b'1': 49,\n",
       " b'2': 50,\n",
       " b'3': 51,\n",
       " b'4': 52,\n",
       " b'5': 53,\n",
       " b'6': 54,\n",
       " b'7': 55,\n",
       " b'8': 56,\n",
       " b'9': 57,\n",
       " b':': 58,\n",
       " b';': 59,\n",
       " b'<': 60,\n",
       " b'=': 61,\n",
       " b'>': 62,\n",
       " b'?': 63,\n",
       " b'@': 64,\n",
       " b'A': 65,\n",
       " b'B': 66,\n",
       " b'C': 67,\n",
       " b'D': 68,\n",
       " b'E': 69,\n",
       " b'F': 70,\n",
       " b'G': 71,\n",
       " b'H': 72,\n",
       " b'I': 73,\n",
       " b'J': 74,\n",
       " b'K': 75,\n",
       " b'L': 76,\n",
       " b'M': 77,\n",
       " b'N': 78,\n",
       " b'O': 79,\n",
       " b'P': 80,\n",
       " b'Q': 81,\n",
       " b'R': 82,\n",
       " b'S': 83,\n",
       " b'T': 84,\n",
       " b'U': 85,\n",
       " b'V': 86,\n",
       " b'W': 87,\n",
       " b'X': 88,\n",
       " b'Y': 89,\n",
       " b'Z': 90,\n",
       " b'[': 91,\n",
       " b'\\\\': 92,\n",
       " b']': 93,\n",
       " b'^': 94,\n",
       " b'_': 95,\n",
       " b'`': 96,\n",
       " b'a': 97,\n",
       " b'b': 98,\n",
       " b'c': 99,\n",
       " b'd': 100,\n",
       " b'e': 101,\n",
       " b'f': 102,\n",
       " b'g': 103,\n",
       " b'h': 104,\n",
       " b'i': 105,\n",
       " b'j': 106,\n",
       " b'k': 107,\n",
       " b'l': 108,\n",
       " b'm': 109,\n",
       " b'n': 110,\n",
       " b'o': 111,\n",
       " b'p': 112,\n",
       " b'q': 113,\n",
       " b'r': 114,\n",
       " b's': 115,\n",
       " b't': 116,\n",
       " b'u': 117,\n",
       " b'v': 118,\n",
       " b'w': 119,\n",
       " b'x': 120,\n",
       " b'y': 121,\n",
       " b'z': 122,\n",
       " b'{': 123,\n",
       " b'|': 124,\n",
       " b'}': 125,\n",
       " b'~': 126,\n",
       " b'\\x7f': 127,\n",
       " b'\\xc2\\x80': 128,\n",
       " b'\\xc2\\x81': 129,\n",
       " b'\\xc2\\x82': 130,\n",
       " b'\\xc2\\x83': 131,\n",
       " b'\\xc2\\x84': 132,\n",
       " b'\\xc2\\x85': 133,\n",
       " b'\\xc2\\x86': 134,\n",
       " b'\\xc2\\x87': 135,\n",
       " b'\\xc2\\x88': 136,\n",
       " b'\\xc2\\x89': 137,\n",
       " b'\\xc2\\x8a': 138,\n",
       " b'\\xc2\\x8b': 139,\n",
       " b'\\xc2\\x8c': 140,\n",
       " b'\\xc2\\x8d': 141,\n",
       " b'\\xc2\\x8e': 142,\n",
       " b'\\xc2\\x8f': 143,\n",
       " b'\\xc2\\x90': 144,\n",
       " b'\\xc2\\x91': 145,\n",
       " b'\\xc2\\x92': 146,\n",
       " b'\\xc2\\x93': 147,\n",
       " b'\\xc2\\x94': 148,\n",
       " b'\\xc2\\x95': 149,\n",
       " b'\\xc2\\x96': 150,\n",
       " b'\\xc2\\x97': 151,\n",
       " b'\\xc2\\x98': 152,\n",
       " b'\\xc2\\x99': 153,\n",
       " b'\\xc2\\x9a': 154,\n",
       " b'\\xc2\\x9b': 155,\n",
       " b'\\xc2\\x9c': 156,\n",
       " b'\\xc2\\x9d': 157,\n",
       " b'\\xc2\\x9e': 158,\n",
       " b'\\xc2\\x9f': 159,\n",
       " b'\\xc2\\xa0': 160,\n",
       " b'\\xc2\\xa1': 161,\n",
       " b'\\xc2\\xa2': 162,\n",
       " b'\\xc2\\xa3': 163,\n",
       " b'\\xc2\\xa4': 164,\n",
       " b'\\xc2\\xa5': 165,\n",
       " b'\\xc2\\xa6': 166,\n",
       " b'\\xc2\\xa7': 167,\n",
       " b'\\xc2\\xa8': 168,\n",
       " b'\\xc2\\xa9': 169,\n",
       " b'\\xc2\\xaa': 170,\n",
       " b'\\xc2\\xab': 171,\n",
       " b'\\xc2\\xac': 172,\n",
       " b'\\xc2\\xad': 173,\n",
       " b'\\xc2\\xae': 174,\n",
       " b'\\xc2\\xaf': 175,\n",
       " b'\\xc2\\xb0': 176,\n",
       " b'\\xc2\\xb1': 177,\n",
       " b'\\xc2\\xb2': 178,\n",
       " b'\\xc2\\xb3': 179,\n",
       " b'\\xc2\\xb4': 180,\n",
       " b'\\xc2\\xb5': 181,\n",
       " b'\\xc2\\xb6': 182,\n",
       " b'\\xc2\\xb7': 183,\n",
       " b'\\xc2\\xb8': 184,\n",
       " b'\\xc2\\xb9': 185,\n",
       " b'\\xc2\\xba': 186,\n",
       " b'\\xc2\\xbb': 187,\n",
       " b'\\xc2\\xbc': 188,\n",
       " b'\\xc2\\xbd': 189,\n",
       " b'\\xc2\\xbe': 190,\n",
       " b'\\xc2\\xbf': 191,\n",
       " b'\\xc3\\x80': 192,\n",
       " b'\\xc3\\x81': 193,\n",
       " b'\\xc3\\x82': 194,\n",
       " b'\\xc3\\x83': 195,\n",
       " b'\\xc3\\x84': 196,\n",
       " b'\\xc3\\x85': 197,\n",
       " b'\\xc3\\x86': 198,\n",
       " b'\\xc3\\x87': 199,\n",
       " b'\\xc3\\x88': 200,\n",
       " b'\\xc3\\x89': 201,\n",
       " b'\\xc3\\x8a': 202,\n",
       " b'\\xc3\\x8b': 203,\n",
       " b'\\xc3\\x8c': 204,\n",
       " b'\\xc3\\x8d': 205,\n",
       " b'\\xc3\\x8e': 206,\n",
       " b'\\xc3\\x8f': 207,\n",
       " b'\\xc3\\x90': 208,\n",
       " b'\\xc3\\x91': 209,\n",
       " b'\\xc3\\x92': 210,\n",
       " b'\\xc3\\x93': 211,\n",
       " b'\\xc3\\x94': 212,\n",
       " b'\\xc3\\x95': 213,\n",
       " b'\\xc3\\x96': 214,\n",
       " b'\\xc3\\x97': 215,\n",
       " b'\\xc3\\x98': 216,\n",
       " b'\\xc3\\x99': 217,\n",
       " b'\\xc3\\x9a': 218,\n",
       " b'\\xc3\\x9b': 219,\n",
       " b'\\xc3\\x9c': 220,\n",
       " b'\\xc3\\x9d': 221,\n",
       " b'\\xc3\\x9e': 222,\n",
       " b'\\xc3\\x9f': 223,\n",
       " b'\\xc3\\xa0': 224,\n",
       " b'\\xc3\\xa1': 225,\n",
       " b'\\xc3\\xa2': 226,\n",
       " b'\\xc3\\xa3': 227,\n",
       " b'\\xc3\\xa4': 228,\n",
       " b'\\xc3\\xa5': 229,\n",
       " b'\\xc3\\xa6': 230,\n",
       " b'\\xc3\\xa7': 231,\n",
       " b'\\xc3\\xa8': 232,\n",
       " b'\\xc3\\xa9': 233,\n",
       " b'\\xc3\\xaa': 234,\n",
       " b'\\xc3\\xab': 235,\n",
       " b'\\xc3\\xac': 236,\n",
       " b'\\xc3\\xad': 237,\n",
       " b'\\xc3\\xae': 238,\n",
       " b'\\xc3\\xaf': 239,\n",
       " b'\\xc3\\xb0': 240,\n",
       " b'\\xc3\\xb1': 241,\n",
       " b'\\xc3\\xb2': 242,\n",
       " b'\\xc3\\xb3': 243,\n",
       " b'\\xc3\\xb4': 244,\n",
       " b'\\xc3\\xb5': 245,\n",
       " b'\\xc3\\xb6': 246,\n",
       " b'\\xc3\\xb7': 247,\n",
       " b'\\xc3\\xb8': 248,\n",
       " b'\\xc3\\xb9': 249,\n",
       " b'\\xc3\\xba': 250,\n",
       " b'\\xc3\\xbb': 251,\n",
       " b'\\xc3\\xbc': 252,\n",
       " b'\\xc3\\xbd': 253,\n",
       " b'\\xc3\\xbe': 254,\n",
       " b'\\xc3\\xbf': 255,\n",
       " b' t': 256,\n",
       " b'he': 257,\n",
       " b' a': 258,\n",
       " b' s': 259,\n",
       " b' w': 260,\n",
       " b' the': 261,\n",
       " b'nd': 262,\n",
       " b'ed': 263,\n",
       " b' b': 264,\n",
       " b' to': 265,\n",
       " b' and': 266,\n",
       " b' h': 267,\n",
       " b' f': 268,\n",
       " b' T': 269,\n",
       " b'in': 270,\n",
       " b' wa': 271,\n",
       " b're': 272,\n",
       " b'it': 273,\n",
       " b'ou': 274,\n",
       " b' l': 275,\n",
       " b' d': 276,\n",
       " b' c': 277,\n",
       " b' p': 278,\n",
       " b'ay': 279,\n",
       " b' m': 280,\n",
       " b'er': 281,\n",
       " b' was': 282,\n",
       " b' The': 283,\n",
       " b'om': 284,\n",
       " b' he': 285,\n",
       " b'is': 286,\n",
       " b' n': 287,\n",
       " b'im': 288,\n",
       " b'ar': 289,\n",
       " b'on': 290,\n",
       " b' sa': 291,\n",
       " b'll': 292,\n",
       " b'id': 293,\n",
       " b' ha': 294,\n",
       " b' g': 295,\n",
       " b'at': 296,\n",
       " b' S': 297,\n",
       " b'ing': 298,\n",
       " b'<|endoftext|>': 299}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ecf8424-754d-448b-9ee2-8e4ccd6dbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# start_time =  time.time()\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     raw_text = f.read()\n",
    "#     f.close()\n",
    "# it = pre_tokenization(raw_text)\n",
    "# word_count1 = count_word_freq(it)\n",
    "# end_time = time.time()\n",
    "# print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c8647c8e-0522-497c-89af-6e8e7ccc3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 串行实现\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# word_freq = dict()\n",
    "\n",
    "# with open(file_path, \"rb\") as f:\n",
    "#     num_processes = 4\n",
    "#     boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "    \n",
    "#     # The following is a serial implementation, but you can parallelize this\n",
    "#     # by sending each start/end pair to a set of processes.\n",
    "#     for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "#         f.seek(start)\n",
    "#         chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "#         # Run pre-tokenization on your chunk and store the counts for each pre-token\n",
    "#         it = pre_tokenization(chunk)\n",
    "#         word_count = count_word_freq(it)\n",
    "#         word_freq = dict(Counter(word_freq) + Counter(word_count))\n",
    "#     f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "da180a55-c662-4005-9cc9-0df5718c6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def process_chunk(start_end_pair, file_path):\n",
    "    \"\"\"\n",
    "    处理单个文件块并计算词频。\n",
    "    此函数由每个工作进程独立执行。\n",
    "\n",
    "    Args:\n",
    "        start_end_pair (tuple): 包含块的起始和结束字节位置的元组 (start, end)。\n",
    "        file_path (str): 要读取的文件的路径。\n",
    "\n",
    "    Returns:\n",
    "        Counter: 该块内单词及其频率的 Counter 对象。\n",
    "    \"\"\"\n",
    "    start, end = start_end_pair\n",
    "    # 每个进程必须独立打开文件，因为文件对象不能在进程间共享。\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        # 对你的数据块运行预分词并存储词频\n",
    "        it = pre_tokenization(chunk)\n",
    "        word_count = count_word_freq(it)\n",
    "        return Counter(word_count)\n",
    "\n",
    "def parallel_word_count(file_path, num_processes=4):\n",
    "    \"\"\"\n",
    "    并行计算文件中单词的频率。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 目标文件的路径。\n",
    "        num_processes (int): 要使用的进程数。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含所有单词及其总频率的字典。\n",
    "    \"\"\"\n",
    "    # 1. 在主进程中确定所有数据块的边界\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "    # 创建一个 (start, end) 元组的列表，供工作进程处理\n",
    "    chunks = list(zip(boundaries[:-1], boundaries[1:]))\n",
    "\n",
    "    # 2. 创建一个工作进程池\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        # 使用 functools.partial 创建一个新函数\n",
    "        # 这样可以将 file_path 参数固定，pool.map 调用时只需传递变化的 chunk 参数\n",
    "        worker_func = partial(process_chunk, file_path=file_path)\n",
    "\n",
    "        # 3. 将任务分配给进程池并收集结果\n",
    "        # pool.map 会将 chunks 列表中的每个元素作为参数传递给 worker_func，并并行执行\n",
    "        list_of_counts = pool.map(worker_func, chunks)\n",
    "\n",
    "    # 4. 合并所有进程返回的局部结果\n",
    "    final_word_freq = Counter()\n",
    "    for count_obj in list_of_counts:\n",
    "        final_word_freq.update(count_obj)\n",
    "        \n",
    "    return dict(final_word_freq)\n",
    "\n",
    "# --- 主程序入口 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 注意：在 Windows 和 macOS 上，多进程代码必须放在 `if __name__ == \"__main__\":` 块中。\n",
    "    # file_path = \"\" \n",
    "    \n",
    "    # 调用并行处理函数\n",
    "    word_freq = parallel_word_count(file_path, num_processes=8) # 可根据 CPU 核心数调整\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c7ee664f-ffae-4877-9878-acc8c17d3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert word_count1 == dict(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cd8a6-a539-40fd-a7fe-2539c8c0b351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
