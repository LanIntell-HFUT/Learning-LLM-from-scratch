{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4566380-0037-40cd-b2db-ded6a5dc45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "PAT=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0e4371b8-5873-464d-a7c6-d579ccd6802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Iterable,Iterator\n",
    "# class Tokenizer:\n",
    "#     def __init__(self, vocab: dict[int, bytes], merges: list[tuple[bytes, bytes]], special_tokens: list[str] | None = None):\n",
    "#         self.vocab=vocab\n",
    "#         self.merges=merges\n",
    "#         self.special_tokens=special_tokens\n",
    "\n",
    "#     def from_files(cls, vocab_filepath:str, merges_filepath:str, special_tokens: list[str] | None = None):\n",
    "#         return\n",
    "\n",
    "#     def encode(self,text: str) -> list[int]:\n",
    "#         return\n",
    "#     def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]: \n",
    "#         return\n",
    "\n",
    "#     def decode(self, ids: list[int]) -> str:\n",
    "#         return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "99ab1472-d367-4f2f-9f07-65d7f423f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer_TinyStories-train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "vocab = data['vocab']\n",
    "merges = data['merges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc2ccb02-1eb9-40e5-b8ad-b0c47b70ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=Tokenizer(vocab,merges,[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "69f34a78-a62f-4326-a97b-25a12aa3dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"the cat ate<|endoftext|>\"\n",
    "voc = {\n",
    "0: b' ',\n",
    "1: b'a',\n",
    "2: b'c',\n",
    "3: b'e',\n",
    "4: b'h',\n",
    "5: b't',\n",
    "6: b'th',\n",
    "7: b' c',\n",
    "8: b' a',\n",
    "9: b'the',\n",
    "10:b' at',\n",
    "11:b'<|endoftext|>'\n",
    "}\n",
    "merges = [\n",
    "(b't', b'h'),\n",
    "(b' ', b'c'),\n",
    "(b' ', b'a'),\n",
    "(b'th', b'e'),\n",
    "(b' a', b't')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2b3ab-104f-4e78-b65c-4d575543de98",
   "metadata": {},
   "source": [
    "第一步 预编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "11b2048c-df2d-405f-a222-abac55940b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ' cat', ' ate']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bpe_train import pre_tokenization\n",
    "it = pre_tokenization(s,special_tokens=[\"<|endoftext|>\"])\n",
    "pre_tokens=[]\n",
    "for i in it:\n",
    "    pre_tokens.append(i.group())\n",
    "pre_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "912fcebc-bd54-4e5f-9934-cec35fcb415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_token(token):\n",
    "    splited=[]\n",
    "    for c in token:\n",
    "        splited.append(c.encode(\"utf-8\"))\n",
    "    return splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "761856b0-755f-4d09-ac85-047521a4d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_merge(token,merges): # token='the'\n",
    "    chars=split_token(token) # chars=b't',b'h',b'e'\n",
    "    i=0\n",
    "    while i<len(chars) - 1:\n",
    "        # print(chars[i],chars[i+1])\n",
    "        if (chars[i],chars[i+1]) in merges:\n",
    "            combined = chars[i]+chars[i+1]\n",
    "            # print(combined)\n",
    "            chars = chars[:i] + [combined] + chars[i+2:]\n",
    "        else:\n",
    "            # print(f\"{(chars[i],chars[i+1])} not in merges\")\n",
    "            i = i+1\n",
    "\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75665194-c600-4acc-98c5-d3dd5af721cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_merged_tokens_to_ids(merged_tokens,vocab):\n",
    "    token_ids=[]\n",
    "    for token in merged_tokens:\n",
    "        for k,v in vocab.items():\n",
    "            if token==v:\n",
    "                token_ids.append(k)\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4073769-c7c6-45c6-b1b6-bad498daa21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "19ecf4e7-fa18-4b47-940f-6c62b8554086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'the']\n",
      "[b' c', b'a', b't']\n",
      "[b' at', b'e']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 7, 1, 5, 10, 3]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids=[]\n",
    "for token in pre_tokens:\n",
    "    merged_tokens = perform_merge(token,merges)\n",
    "    print(merged_tokens)\n",
    "    ids = map_merged_tokens_to_ids(merged_tokens,voc)\n",
    "    for i in ids:\n",
    "        token_ids.append(i)\n",
    "token_ids        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "920a8cd6-07dd-49c9-8ac8-41b027d9fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def pre_tokenization_with_specials(raw_text: str, \n",
    "                                   special_tokens=[\"<|endoftext|>\"], \n",
    "                                   PAT=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"):\n",
    "    # 构造正则，将特殊符号作为优先匹配\n",
    "    special_pattern = \"|\".join(re.escape(t) for t in special_tokens)\n",
    "    combined_pattern = f\"({special_pattern})|({PAT})\"\n",
    "\n",
    "    # 匹配：如果 group(1) 不为空，就是特殊符号；否则是普通词\n",
    "    tokens_with_type = []\n",
    "    for match in re.finditer(combined_pattern, raw_text):\n",
    "        if match.group(1) is not None:  \n",
    "            tokens_with_type.append((match.group(1), True))   # True 表示是特殊token\n",
    "        elif match.group(2) is not None:\n",
    "            tokens_with_type.append((match.group(2), False))  # False 表示普通token\n",
    "    return tokens_with_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "df65d485-5ac2-4dbc-9e63-cdfee860567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab: dict[int, bytes], merges: list[tuple[bytes, bytes]], special_tokens: list[str] | None = None):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens or []\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        token_ids = []\n",
    "        for token, is_special in pre_tokenization_with_specials(text, self.special_tokens):\n",
    "            if is_special:\n",
    "                # 直接查 vocab\n",
    "                for k, v in self.vocab.items():\n",
    "                    if v.decode() == token:\n",
    "                        token_ids.append(k)\n",
    "                        break\n",
    "            else:\n",
    "                merged_tokens = perform_merge(token, self.merges)\n",
    "                ids = map_merged_tokens_to_ids(merged_tokens, self.vocab)\n",
    "                token_ids.extend(ids)\n",
    "        return token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a75992e-20d2-40fd-abb9-e64956b75b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=Tokenizer(voc,merges,[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "05cfd71f-0bf4-4213-82e3-8f955280eacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 7, 1, 5, 10, 3, 11]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dec1576-dbb5-46ef-85e7-380dc929b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [9, 7, 1, 5, 10, 3, 11]\n",
      "Decoded Text: the cat ate<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Iterator\n",
    "import regex as re\n",
    "\n",
    "def split_token(token: str):\n",
    "    \"\"\"将字符串 token 拆成 utf-8 bytes 列表\"\"\"\n",
    "    return [c.encode(\"utf-8\") for c in token]\n",
    "\n",
    "def perform_merge(token: str, merges: list[tuple[bytes, bytes]]):\n",
    "    \"\"\"执行 BPE 合并\"\"\"\n",
    "    chars = split_token(token)\n",
    "    i = 0\n",
    "    while i < len(chars) - 1:\n",
    "        if (chars[i], chars[i+1]) in merges:\n",
    "            combined = chars[i] + chars[i+1]\n",
    "            chars = chars[:i] + [combined] + chars[i+2:]\n",
    "        else:\n",
    "            i += 1\n",
    "    return chars\n",
    "\n",
    "def map_merged_tokens_to_ids(merged_tokens: list[bytes], vocab: dict[int, bytes]):\n",
    "    \"\"\"将合并后的 bytes token 映射到 ID\"\"\"\n",
    "    token_ids = []\n",
    "    for token in merged_tokens:\n",
    "        for k, v in vocab.items():\n",
    "            if token == v:\n",
    "                token_ids.append(k)\n",
    "                break\n",
    "    return token_ids\n",
    "\n",
    "def pre_tokenization_with_specials(raw_text: str, \n",
    "                                   special_tokens: list[str], \n",
    "                                   PAT=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"):\n",
    "    \"\"\"先识别特殊符号，再正则分词\"\"\"\n",
    "    if not special_tokens:\n",
    "        special_tokens = []\n",
    "    special_pattern = \"|\".join(re.escape(t) for t in special_tokens)\n",
    "    if special_pattern:\n",
    "        combined_pattern = f\"({special_pattern})|({PAT})\"\n",
    "    else:\n",
    "        combined_pattern = f\"({PAT})\"\n",
    "\n",
    "    tokens_with_type = []\n",
    "    for match in re.finditer(combined_pattern, raw_text):\n",
    "        if special_tokens and match.group(1) is not None:  \n",
    "            tokens_with_type.append((match.group(1), True))   # 特殊token\n",
    "        else:\n",
    "            tok = match.group(2) if special_tokens else match.group(1)\n",
    "            if tok:\n",
    "                tokens_with_type.append((tok, False))         # 普通token\n",
    "    return tokens_with_type\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab: dict[int, bytes], merges: list[tuple[bytes, bytes]], special_tokens: list[str] | None = None):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens or []\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: list[str] | None = None):\n",
    "        # 加载 vocab\n",
    "        vocab = {}\n",
    "        with open(vocab_filepath, \"rb\") as vf:\n",
    "            for i, line in enumerate(vf):\n",
    "                vocab[i] = line.strip()\n",
    "\n",
    "        # 加载 merges\n",
    "        merges = []\n",
    "        with open(merges_filepath, \"rb\") as mf:\n",
    "            for line in mf:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    merges.append((parts[0], parts[1]))\n",
    "\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        token_ids = []\n",
    "        for token, is_special in pre_tokenization_with_specials(text, self.special_tokens):\n",
    "            if is_special:\n",
    "                # 特殊符号直接映射\n",
    "                for k, v in self.vocab.items():\n",
    "                    if v.decode() == token:\n",
    "                        token_ids.append(k)\n",
    "                        break\n",
    "            else:\n",
    "                merged_tokens = perform_merge(token, self.merges)\n",
    "                ids = map_merged_tokens_to_ids(merged_tokens, self.vocab)\n",
    "                token_ids.extend(ids)\n",
    "        return token_ids\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]: \n",
    "        for text in iterable:\n",
    "            for token_id in self.encode(text):\n",
    "                yield token_id\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        tokens = [self.vocab[i].decode(\"utf-8\") for i in ids]\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "\n",
    "# ===== 测试 =====\n",
    "if __name__ == \"__main__\":\n",
    "    s = \"the cat ate<|endoftext|>\"\n",
    "\n",
    "    voc = {\n",
    "        0: b' ',\n",
    "        1: b'a',\n",
    "        2: b'c',\n",
    "        3: b'e',\n",
    "        4: b'h',\n",
    "        5: b't',\n",
    "        6: b'th',\n",
    "        7: b' c',\n",
    "        8: b' a',\n",
    "        9: b'the',\n",
    "        10: b' at',\n",
    "        11: b'<|endoftext|>'\n",
    "    }\n",
    "    merges = [\n",
    "        (b't', b'h'),\n",
    "        (b' ', b'c'),\n",
    "        (b' ', b'a'),\n",
    "        (b'th', b'e'),\n",
    "        (b' a', b't')\n",
    "    ]\n",
    "\n",
    "    tokenizer = Tokenizer(voc, merges, special_tokens=[\"<|endoftext|>\"])\n",
    "    ids = tokenizer.encode(s)\n",
    "    print(\"Encoded IDs:\", ids)\n",
    "    print(\"Decoded Text:\", tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f55c03-4ef1-44ec-9c29-b765b4e03eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer_TinyStories-train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "vocab = data['vocab']\n",
    "merges = data['merges']\n",
    "tokenizer = Tokenizer(vocab, merges, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34767765-9373-4759-bbf4-fd548cdda530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [73, 271, 346, 265, 344, 296, 258, 2880, 8945, 114, 780, 114, 46]\n",
      "Decoded Text: I want to eat a hamburger.\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"I want to eat a hamburger.\")\n",
    "print(\"Encoded IDs:\", ids)\n",
    "print(\"Decoded Text:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a30f8-1a82-452b-97cc-a6804874c5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
