# 从零构建大模型总结

## 1 大语言模型概述与数据处理-张钰轩

### 1.1 大模型概念

<img src="assets/image-20251018093744566.png" alt="image-20251018093744566" style="zoom: 50%;" />

​	

### 1.2 大模型训练流程

<img src="assets/image-20251018093820045.png" alt="image-20251018093820045" style="zoom:67%;" />

### 1.3 常见的数据集及模型

​	介绍了CommonCrawl、WebText2、Books1、Books2、Wikipedia数据集。

| **模型名称** | **模型参数**     | **训练数据**     | **发布日期** |
| ------------ | ---------------- | ---------------- | ------------ |
| GPT-1        | 1.17亿（117M）   | 4.5GB            | 2018.6.11    |
| GPT-2        | 15亿（1.5B）     | 40GB             | 2019.2.14    |
| GPT-3        | 1750亿（175B）   | 45TB             | 2020.5.28    |
| GPT-3.5      | 1750亿（175B）   | 无               | 2022.3.15    |
| GPT-4        | ~1.7万亿（1.7T） | 数十万亿（PB级） | 2023.3.14    |

### 1.4 分词器

​	在每一轮迭代中，只合并当前语料中出现频率最高的一个相邻符号对，通过不断合并，从字符逐步构建出更长的、常见的子词单元，直到词汇表的大小增长到预先设定的目标值将不在预定义词汇表中的单词分解为更小的子词单元甚至单个字符，从而能够处理词汇表之外的单词。

​	相较于传统的BPE，**BBPE**首先将文本编码为UTF-8字节序列，然后执行BPE合并。<img src="assets/image-20251018094322783.png" alt="image-20251018094322783" style="zoom:67%;" />

​	分词器也会带来一些问题，在不思考的情况下，不能拼写单词？同样LLM反转一个单词，也有可能会出现这种情况。所以BPE绝对不是一种完美的分词方法，但是目前也很难说有比BPE更好的方法。

### 1.5 从文本到嵌入的映射

<img src="assets/image-20251018094521903.png" alt="image-20251018094521903" style="zoom:67%;" />

介绍绝对位置编码和绝对位置编码——正弦-余弦编码：

- 绝对位置嵌入，直接与序列中的特定位置相关联。对于输入序列的每个位置，该方法都会向对应词元的嵌入向量中添加一个独特的位置嵌入，以明确指示其在序列中的确切位置。
- 绝对位置编码——正弦-余弦编码原始Transformer论文提出的方法，使用不同频率的正弦和余弦函数。优点：简单有效，理论上有外推能力缺点：**无法直接建模相对位置关系，能处理的长度有限**

介绍了相对位置编码的思想以及旋转位置编码(RoPE)的概念:

​	相对位置嵌入关注的是词元之间的相对位置或距离，而非它们的绝对位置。

<img src="assets/image-20251018094856512.png" alt="image-20251018094856512" style="zoom:67%;" />

## 2 大语言模型架构-万展翼

### 2.1 经典Transformer架构-attention

<img src="assets/image-20251018095138394.png" alt="image-20251018095138394" style="zoom:67%;" />

两种实现注意力机制的方法：

```python
def forward(self, x):
    """
    x.shape = (batch_size, seq_len, d_in)
    """
    batch_size, seq_len, d_in = x.size()

    # 计算三个矩阵
    Q = self.query(x)
    K = self.key(x)
    V = self.value(x)

    # 计算注意力分数
    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(K.shape[-1])

    # 掩码填充
    attention_scores.masked_fill_(
        self.mask[:seq_len, :seq_len], -torch.inf
    )

    # print("attention_scores:", attention_scores)
    attention_weights = torch.softmax(attention_scores, dim=-1)
    attention_weights = self.dropout(attention_weights)
    output = torch.matmul(attention_weights, V)
    return output

# 多头注意力的拼接
def forward(self, x):
    # x.shape = (batch_size, seq_len, d_in)
    head_outputs = [head(x) for head in self.heads]
    # 将多个头的输出拼接在一起
    output = torch.cat(head_outputs, dim=-1)
    return output
```

```
def forward(self, x):
    batch_size, seq_len, d_in = x.size()

    # 计算 Q, K, V，形状为 (batch_size, seq_len, d_out)
    Q = self.query(x)
    K = self.key(x)
    V = self.value(x)

    # 将 Q, K, V 分割成多个头，形状为 (batch_size, num_heads, seq_len, head_dim)
    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
    K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
    V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

    # 计算注意力分数（与单头一致）
    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim))
    attention_scores.masked_fill_(
        self.mask[:seq_len, :seq_len], -torch.inf
    )

    attention_scores = torch.softmax(attention_scores, dim=-1)
    attention_scores = self.dropout(attention_scores)
    output = torch.matmul(attention_scores, V).transpose(1, 2)

    # 将多个头的输出拼接在一起
    output = output.contiguous().view(batch_size, seq_len, self.d_out)
    return self.out(output)

```

​	第一种方法使用堆叠的方式实现，第二种方法主要是通过矩阵的变形实现d_model = num_heads * head_dim。

### 2.3 比较GPT模型中decoder与传统的transformer差异

<img src="assets/image-20251018100527762.png" alt="image-20251018100527762" style="zoom:67%;" />

<img src="assets/image-20251018100534476.png" alt="image-20251018100534476" style="zoom:67%;" />

​	GPT模型中decoder与传统的transformer的一个不同点就是：归一化的位置前者采用pre-Norm, 而后者采用post-Norm；这种设计的优势在于：**梯度至少能通过残差路径稳定地回传，这确保了最底层的网络也能接收到有效的梯度信号进行更新。**

### 2.4 采用TopK以及温度缩放

```python
def generate(model, input_ids, max_length, context_size, tok_k=None, temperature=1.0):
    model.eval()  # 设置模型为eval模式
    for _ in range(max_length):
        # 只保留最后 context_size 个 token，确保输入不超窗口
        input_ids = input_ids[:, -context_size:]

        # 计算下一个 token 的概率分布
        with torch.no_grad():
            logits = model(input_ids)

        # 取最后一个位置的输出
        next_token_logits = logits[:, -1, :]  # (batch_size, vocab_size)

        # ---------- top-k 采样 ----------
        if tok_k is not None:
            topk_logits, topk_indexes = torch.topk(next_token_logits, tok_k, dim=-1)
            min_values = topk_logits[:, -1].unsqueeze(-1)
            # 将不在 top-k 中的 token 的 logit 置为 -inf
            next_token_logits = torch.where(
                next_token_logits < min_values,
                torch.tensor(float('-inf')).to(next_token_logits.device),
                next_token_logits
            )

        # ---------- 温度采样 ----------
        if temperature != 0.0 and tok_k is not None:
            # 当温度为 1 时，等价于普通 softmax
            next_token_logits = next_token_logits / temperature
            probs = torch.softmax(next_token_logits, dim=-1)
            # 多项式随机采样
            next_tokens = torch.multinomial(probs, num_samples=1)
        else:
            # 不使用采样策略时，直接取最大概率 token
            next_token_logits = torch.softmax(next_token_logits, dim=-1)
            next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)

        # 拼接新生成的 token
        input_ids = torch.cat([input_ids, next_tokens], dim=-1)

    return input_ids

```

### 2.5 SwiGLU激活函数

​	SwiGLU（Swish-Gated Linear Unit）是一种结合了Swish和GLU（Gated Linear Unit）特点的激活函数。

<img src="assets/image-20251018100934545.png" alt="image-20251018100934545" style="zoom:67%;" />

```python
def run_swiglu(
    d_model: int,
    d_ff: int,
    w1_weight: Float[Tensor, "d_ff d_model"],
    w2_weight: Float[Tensor, "d_model d_ff"],
    w3_weight: Float[Tensor, "d_ff d_model"],
    in_features: Float[Tensor, "... d_model"],
) -> Float[Tensor, "... d_model"]:
    """
    实现 SwiGLU 前馈层：
    y = (x @ W1.T) * sigmoid(x @ W1.T) * (x @ W3.T) @ W2.T
    其中 W1、W3 对应前向扩展权重，W2 是输出映射权重。
    """

    # 第一步：线性变换
    x_w1 = in_features @ w1_weight.T
    x_w3 = in_features @ w3_weight.T

    # 第二步：应用 SwiGLU 激活
    return (x_w1 * torch.sigmoid(x_w1) * x_w3) @ w2_weight.T

```

传统的 FFN (Feed Forward Network) 一般是 两层 MLP + 激活函数。**SwiGLU 优势**：

- 1.它本质是门控单元，比单一激活函数（ReLU/GELU）更灵活，可以学习到更复杂的特征。
- 2.使用 Swish（平滑版本的 ReLU），Sigmoid 部分提供了平滑的梯度，让训练更稳定。

### 2.6 RoPE旋转位置编码

​	它是由论文提出的一种能够将相对位置信息依赖集成到 self-attention 中并提升 transformer 架构性能的位置编码方式。和相对位置编码相比，**RoPE 具有更好的外推性**，目前是大模型相对位置编码中应用最广的方式之一。大模型外推性：指的是在训练阶段如果context_size是固定值，那么在预测输入超过了固定值的token，模型将无法处理。

<img src="assets/image-20251018101308884.png" alt="image-20251018101308884" style="zoom:67%;" />

<img src="assets/image-20251018101330572.png" alt="image-20251018101330572" style="zoom:67%;" />

### 2.7 eniops库

​	规约（reduce）：则是在某一维度上进行数学操作：mean、max等，该方法在图像操作上会更多一些（池化），但是相比于mean等操作，前者需要付出时间代价<img src="assets/image-20251018101559199.png" alt="image-20251018101559199" style="zoom:67%;" />

## 3 大语言模型预训练-章新悦

## 4 大语言模型微调 -赵麟敖

### 4.1 分类微调

#### 4.1.1 概念

​	模型被训练来识别一组特定的类别标签，比如在消息中过滤“垃圾消息”和“非垃圾消息”。经过垃圾消息数据分类微调的模型在输入时不需要提供额外的指令。与经过指令微调的模型相比，它**只能回复“**垃圾消息”或“非垃圾消息”。<img src="assets/image-20251018101725372.png" alt="image-20251018101725372" style="zoom:67%;" />

#### 4.1.2 分类微调流程

<img src="assets/image-20251018101856125.png" alt="image-20251018101856125" style="zoom:67%;" />

#### 4.1.3 数据集及相关处理

​	分类微调的数据集是带标签的文本。

<img src="assets/image-20251018101941584.png" alt="image-20251018101941584" style="zoom:67%;" />

```python
# 创建平衡数据集：让 spam 与 ham 数量相同
def create_balanced_dataset(df):
    # 统计 "垃圾消息" 的样本数量
    num_spam = df[df["Label"] == "spam"].shape[0]

    # 随机采样相同数量的 "非垃圾消息"
    ham_subset = df[df["Label"] == "ham"].sample(
        num_spam, random_state=123
    )

    # 合并两个子集，构成平衡数据集
    balanced_df = pd.concat([
        ham_subset,
        df[df["Label"] == "spam"]
    ])

    return balanced_df

# 随机划分数据集为 train / validation / test
def random_split(df, train_frac, validation_frac):
    # 打乱整个 DataFrame
    df = df.sample(
        frac=1, random_state=123
    ).reset_index(drop=True)

    # 计算拆分索引
    train_end = int(len(df) * train_frac)
    validation_end = train_end + int(len(df) * validation_frac)

    # 拆分 DataFrame
    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]

    return train_df, validation_df, test_df

```

​	当我们在处理数据的时候，处理的数据包含了不同的长度，为了保持文本能够按批次处理，有两种方案将所有消息截断到数据集中最短消息的长度或批次长度；**将所有消息填充到数据集中最长消息的长度或批次长度**。保证输入的shape一致。在这里选择第二种方案，**保留所有消息的完整内容，不会有信息的丢失。**<img src="assets/image-20251018102241276.png" alt="image-20251018102241276" style="zoom:67%;" />

#### 4.1.4 模型结构以及微调设置

<img src="assets/image-20251018102318010.png" alt="image-20251018102318010" style="zoom:67%;" />

​	由于分类微调是一个分类任务，以垃圾信息分类这个为例，所以最终得到的输出不是词汇大小而是任务标签的个数，这里是2。

```python
# 固定随机种子，保证结果可复现
torch.manual_seed(123)
# 定义分类类别数量（例如：垃圾邮件 vs 非垃圾邮件）
num_classes = 2
# 在模型上添加一个线性分类头
model.out_head = nn.Linear(
    in_features=BASE_CONFIG["emb_dim"],  # 输入维度等于模型的嵌入维度
    out_features=num_classes              # 输出维度等于类别数
)
```

​	在基于神经网络的语言模型中，较低层通常捕捉基本的语言结构和语义，适用于广泛的任务和数据集，**最后几层更侧重于捕捉细微的语言模式和特定任务的特征。**因此，只微调最后几层通常就足以将模型适应到新任务。同时在计算上也更加高效。

```python
for param in model.parameters():
	param.requires_grad = False
```

​	从技术上讲，仅训练刚刚添加的输出层就足够了。然而，实验中发现，**微调额外的层可以显著提升模型的预测性能。**

```py
for param in model.trf blocks[-1].parameters():
    param.requires_grad = True
for param in model.final norm.parameters():
    param.requires_grad = True
```

​	最后也就是训练**最后一个Transformer Block、final_norm以及输出层**。

#### 4.1.5 评估

​	由于输出的只是简单的标签类型，所以只需要根据真实标签来得到分类的准确率即可进行评估。

### 4.2 指令微调

#### 4.2.1 指令微调的概念与流程

​	实现大模型能够遵循人类的指令进行输出。<img src="assets/image-20251018103300352.png" alt="image-20251018103300352" style="zoom:67%;" />

<img src="assets/image-20251018103355339.png" alt="image-20251018103355339" style="zoom:67%;" />

#### 4.2.2 数据集及相关处理

​	指令微调的数据集是**指令+原始输入+期望输入**，来训练模型能够根据特定的指令进行回答操作。

<img src="assets/image-20251018103617116.png" alt="image-20251018103617116" style="zoom:67%;" />

​	为了让模型建立“任务语境”，我们必须显式地给出**角色信号（user/assistant）\**或\**任务边界提示（### Instruction/### Response）**。

<img src="assets/image-20251018103831659.png" alt="image-20251018103831659" style="zoom:67%;" />

​		由于因为每条样本长度不同，文本转化为词元ID之后，和分类微调一样，每个batch需要用结束词元ID**补齐到相同长度**，并在标签中，将不需要训练的部分（例如 padding、指令文本）**替换为 `-100`**。

<img src="assets/image-20251018103955483.png" alt="image-20251018103955483" style="zoom:67%;" />

目标词元ID是什么：

- 与我们预训练大语言模型时的做法相似，目标词元ID 与输入词元ID 相对应，但向**左移动了一个位置**。
- 使得大语言模型能够学习**如何预测序列中的下一个词元**。

为什么用-100去填充：

- 在计算交叉熵损失的时候默认设置ignore_index=-100这意味着它会忽略标记为-100 的目标，从而**不影响loss**。

#### 4.1.3 模型结构以及微调设置

​	由于指令微调是基于文本生成的，所以最后的输出层是不需要修改的，依然是线性层从768映射到50257，微调冻结的参数也和分类微调基本一致。

​	微调模型时，采用的train_loop与之前GPT模型一致，直接调用即可。

```
# 固定随机种子，确保结果可复现
torch.manual_seed(123)

# 定义优化器（AdamW 是 Transformer 常用优化器）
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.00005,          # 学习率
    weight_decay=0.1     # 权重衰减，用于正则化
)

# 训练轮数
num_epochs = 2

# 调用自定义的训练函数
train_losses, val_losses, tokens_seen = train_model_simple(
    model,
    train_loader,
    val_loader,
    optimizer,
    device,
    num_epochs=num_epochs,        # 总训练轮数
    eval_freq=5,                  # 每5个 epoch 评估一次
    eval_iter=5,                  # 每次评估抽5个 batch
    start_context=format_input(val_data[0]),  # 用于评估时生成示例
    tokenizer=tokenizer
)
```

<img src="assets/image-20251018105520038.png" alt="image-20251018105520038" style="zoom:67%;" />

#### 4.1.4 评估

​	在 SFT 过程中，我们仍然把它视作**语言建模任务**，因此模型的评估目标是loss等；对于开放式指令（如创作、对话、代码生成、推理），模型的好坏很难用指标量化，因此一般用**人工打分或使用其他大语言模型**（如GPT-4）来自动评估回复的对话基准，比如AlpacaEval。

<img src="assets/image-20251018110109563.png" alt="image-20251018110109563" style="zoom:67%;" />

## Q&A

1、为什么选择输出的最后一个维度做预测？

​	以为decoder only的架构，在进行输出的过程中只有在计算最后一个词元的时候才能获取全部的上下文，所以最后一个维度的输出能够涵盖所有的上下文信息。

2、topk和温度缩放

​	topk和温度缩放可以是两个策略也可以是整体使用，首先温度缩放是对输出的概率分布进行缩放，温度越高分布更均匀多样性更低，反之多样性更高。topk技术则是将输出的概率分布限制在topk个token上，topk采样之后可以对最后的topk的token进行softmax操作保证他们概率和为1，之后对topk按照其概率分布进行随机采样，同时温度缩放可以对最后的概率分布进行一个缩放。

3、topk和topp

​	topk如上述情况，topp则是增加一个概率阈值保证采样的一个概率超过某个阈值。

4、分类微调和用指令微调做分类任务有什么区别？

​	1.数据集上，分类微调用的是带标签的文本，指令微调则是指令+原始输入+期望输入。

​	2.模型结构上，分类微调要得到的是最终的分类结果，所以最后的输出的维度应该是分类结果的种数，而指令微调做的分类任务输出的仍然是语言，所以输出维度不会发生改变。另外，分类微调关注的是训练样本中的标签，所以填充文本补齐长度之后多余的填充不用替换为-100，而指令微调需要把对于的填充替换为-100，来减少对交叉熵损失的影响，便于更准确的更新参数。

​	3.输出结果方面，分类微调输出的是分类结果的标签，而指令微调做分类任务由于仍然是语言任务，所以输出的是对分类结果的语言描述。